{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Python - Parkinson's Disease Classification Data Analysis\n",
    "\n",
    "In this exercise, we are going to do data analysis with Python and Pandas. As this is the first \"real\" exercise, we will add guidance for some of the tasks.\n",
    "\n",
    "1. Warmup - Generate 100 samples from $\\mathcal{N}(0,1)$ (`np.random.randn`). Compute a 95% CI for the CDF. Plot the true CDF, the CDF estimation and the CI in a single plot. To estimate $\\hat{F}_n$ use a histogram (`np.histogram`). Repeat this $K=1000$ times and compute the percentage of time that the interval contained the CDF (print the value) . In addition, plot in another single figure the *true* CDF, and the best and worst experiments (use $\\max_x|F(x) - \\hat{F}_n(x)|$ as quality measure).\n",
    "    * To compare np arrays element-wise use `np.less_equal(x1, x2 + eps), np.greater_equal(x1, x2 - eps)`, use `.all()` to verify if all the comaprisons were `True`.\n",
    "    \n",
    "We are now going to perform some real data analysis on the \"Parkinson's Disease Classification Data Set\": the data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at the Department of Neurology in CerrahpaÅŸa Faculty of Medicine, Istanbul University. The control group consists of 64 healthy individuals (23 men and 41 women) with ages varying between 41 and 82. During the data collection process, the microphone is set to 44.1 KHz and following the examination, the sustained phonation of the vowel /a/ was collected from each subject with three repetitions. \n",
    "\n",
    "The features are various speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment.\n",
    "\n",
    "2. Load the data with pandas, drop the 'id' column, take a sample ($k=10$, `dataframe.sample(k)`) and view it.\n",
    "    * The filename is `pd_speech_features.csv`.\n",
    "3. Compute the empirical correlation between all pairs of features. Show the results both in a heatmap.\n",
    "    * Use pandas `.corr()` to calculate the correaltion, and `plt.imshow()` to view the heatmap (2 heatmaps, one for the correlation and one for the absolute correlation). Add a color bar using `plt.colorbar()`\n",
    "4. Print the top-20 most correlated features. Follow this steps:\n",
    "    * Take the lower triangle of the correlation matrix (as it is symmetrical and we don't care about $Corr(X_i,X_i)$). Use `np.tril()`\n",
    "    * Consider only positive correlation (because negative correlation has a different, useful meaning). You can do that by `X = X[X >0]`.\n",
    "    * From here, these are recommended steps, feel free to achieve the goal in a different way.\n",
    "        * Assignment to a pandas DataFrame: `X.loc[:,:] = np.(...)`\n",
    "        * Unstacking the DataFrame (creates a new pivot, read the doc): `df.unstack()`\n",
    "        * Sorting: `df.sort()`\n",
    "\n",
    "5. What is the meaning when 2 different features are highly correlated? From a machine learning perspective, can a classifier learn new insights from highly-correlated features? In your answer, address the process of \"feature selection\" in ML (usually performed as a pre-processing step).\n",
    "\n",
    "6. Compute the **in-class** correlation between features. Plot a heat map for each class. Address the differences between the heat maps.\n",
    "\n",
    "7. Consider the features 'numPulses' and 'app_entropy_log_5_coef'. We wish to calculate a 95% confidence interval for the correlation between these features. We will use *Bootstrapping* and the *Chebyshev inequality* (as in Tutorial 2).\n",
    "    * Implement the bootstrap algorithm to calculate the standard deviation ($\\sigma$) of the correlation.\n",
    "        * You can use the algorithm from the tutorial, but you have to modify it to support 2 arrays.\n",
    "        * The algorithm will output the empirical correlation of the two input features, and a bootstrap estimation for the std ($\\sigma$). Use `K=300` bootstrap samples and `m=100` experiments.\n",
    "        * Tips:\n",
    "            * To get values for two columns: `data[['numPulses', 'app_entropy_log_5_coef']]`\n",
    "            * To calculate correlation, check out `numpy.corrcoef`.\n",
    "    * Use $\\sigma$ to calculate a 95% CI using Chebyshev inequality.\n",
    "        * Remember to to normalize by the size of the data ($N$).\n",
    "    The final print should look something like that: `95% confidence interval for the correlation: *** ± ***`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import helpers as h\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['font.size'] = 10\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "N = 100\n",
    "x = np.arange(-3,3+0.001,0.001)\n",
    "x.shape\n",
    "mu = 0\n",
    "sigma = 1\n",
    " \n",
    "true_cdf = norm.cdf(x, mu, sigma)\n",
    "\n",
    "k = 1000\n",
    "count = 0 # counts the number of times the true cdf is inside the CI\n",
    "norm_list = []\n",
    "for i in range(k):\n",
    "    # First calc F estimated and CI  and check if F real is inside the CI\n",
    "    F_n, F_ci_down, F_ci_up = h.calc_plot_CI(0.05, N)\n",
    "    count += 1*((np.greater_equal(true_cdf, F_ci_down).all()) and (np.less_equal(true_cdf, F_ci_up).all()))\n",
    "\n",
    "    # Now loof for the best and worst estimation of F\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"The precantage of the estimators that were inside the CI is: {count/k * 100:.2f}%\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(x, true_cdf, label='true cdf')\n",
    "ax.legend()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The precantage of the estimators that were inside the CI is: 95.50%\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5670323f70>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}